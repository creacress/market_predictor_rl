import os
import time
import pandas as pd
import logging
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from trainers.env_safran_rl import SafranTradingEnv

# Logger config
logging.basicConfig(
    filename='training_continuous.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

MODEL_PATH = "models/ppo_safran_trader"

while True:
    logging.info("üîÅ Lancement de l'entra√Ænement continu")

    # Charger les donn√©es r√©centes (ex: 60 derniers jours)
    df = pd.read_parquet("data/SAF_PA_clean.parquet")
    df = df.tail(60)

    # Cr√©er environnement
    env = DummyVecEnv([lambda: SafranTradingEnv(df)])

    # Charger ou cr√©er le mod√®le
    if os.path.exists(MODEL_PATH + ".zip"):
        model = PPO.load(MODEL_PATH, env=env, verbose=1)
        logging.info("üì• Mod√®le PPO existant charg√©")
    else:
        model = PPO("MlpPolicy", env, verbose=1)
        logging.info("üÜï Nouveau mod√®le PPO initialis√©")

    # R√©entra√Ænement
    model.learn(total_timesteps=10_000)
    model.save(MODEL_PATH)
    logging.info("‚úÖ Mod√®le mis √† jour et sauvegard√©")

    # Pause (ex: 1 jour = 86400s, pour test = 60s)
    logging.info("‚è±Ô∏è Pause avant prochaine it√©ration\n")
    time.sleep(86400)  # pour test : remplace par 60
