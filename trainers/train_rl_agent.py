import pandas as pd
import os
import sys
import logging
import torch

logging.basicConfig(
    filename='training_rl.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from trainers.env_safran_rl import SafranTradingEnv

# Charger les donn√©es de march√© propres
df = pd.read_parquet("data/SAF_PA_clean.parquet")

# Cr√©er l'environnement personnalis√© Gym
env = DummyVecEnv([lambda: SafranTradingEnv(df)])

device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"üìü Device utilis√© : {device}")
print(f"üìü Device utilis√© : {device}")

# Cr√©er le mod√®le PPO
model = PPO("MlpPolicy", env, verbose=1)

# Entra√Æner le mod√®le
model.learn(total_timesteps=100_000)

logging.info("‚úÖ Agent PPO entra√Æn√© et sauvegard√© sous models/ppo_safran_trader")

# Sauvegarder le mod√®le
os.makedirs("models", exist_ok=True)
model.save("models/ppo_safran_trader")

print("‚úÖ Agent PPO entra√Æn√© et sauvegard√© sous models/ppo_safran_trader")