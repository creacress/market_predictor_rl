import pandas as pd
import os
import sys
import logging
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# Logger
logging.basicConfig(
    filename='training_rl.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Setup import local
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from trainers.env_safran_rl import SafranTradingEnv

# ğŸ“¦ Config
MODEL_PATH = "models/ppo_safran_trader"
TOTAL_TIMESTEPS = 100_000

# ğŸ“ˆ Charger les donnÃ©es
df = pd.read_parquet("data/SAF_PA_clean.parquet")

# ğŸ”„ Environnement Gym personnalisÃ©
env = DummyVecEnv([lambda: SafranTradingEnv(df)])

# ğŸ¯ Device
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"ğŸ“Ÿ Device utilisÃ© : {device}")
print(f"ğŸ“Ÿ Device utilisÃ© : {device}")

# âš™ï¸ Charger ou crÃ©er le modÃ¨le
if os.path.exists(MODEL_PATH + ".zip"):
    model = PPO.load(MODEL_PATH, env=env, device=device, verbose=1)
    logging.info("ğŸ“¥ ModÃ¨le PPO existant chargÃ©")
else:
    model = PPO("MlpPolicy", env, verbose=1, device=device)
    logging.info("ğŸ†• Nouveau modÃ¨le PPO initialisÃ©")

# ğŸ§  EntraÃ®nement
model.learn(total_timesteps=TOTAL_TIMESTEPS)
logging.info(f"âœ… EntraÃ®nement terminÃ© : {TOTAL_TIMESTEPS} steps")

# ğŸ’¾ Sauvegarde
os.makedirs("models", exist_ok=True)
model.save(MODEL_PATH)
logging.info(f"ğŸ’¾ ModÃ¨le sauvegardÃ© : {MODEL_PATH}")
